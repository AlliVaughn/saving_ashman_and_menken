{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final - GPT-2 Training :  Text-Generating Model w/ GPU",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JevbS-PyZnMC",
        "colab_type": "text"
      },
      "source": [
        "# **Saving Ashman & Menken: How to Train up an AI Disney Lyracist, long passed**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucQ_yVTFYxZb",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://d23.com/app/uploads/2016/09/780w-463h_092616_oral-history-howard-ashman-1.jpg)\n",
        "\n",
        "##What are you using Machine Learning for? \n",
        "---------------------------------------------\n",
        "Like so many celebrated Disney Lyracists, Howard was extremely overworked and  woefully underpaid!(Kidding!) He passed away in the 1990's but his long-time partner Alan Menken still misses him. Together, they were resposible for some pretty iconic Disney soundtracks. What if Alan could have a version of his old partner back again?  </br>\n",
        "Together they wrote Little Mermaid, Aladdin and Beauty and the Beast. Menken went on to write Pocahontas, Newsies, The Hunchback of Notre Dame, Hercules, Home on the Range, Enchanted, and Tangled, among others.\n",
        "Alan is hoping that AI will help him knock out some new hit Disney songs with Howard's old lyrics flair. \n",
        "\n",
        "\n",
        "## **What we are aiming for:** \n",
        "**Metrics:** Menken once said that his songs should be hummable, meaning catchy, and Ashman was a playwright that also thought that they should sound natural, so that's what we are aiming for!  \n",
        "</br> \n",
        "Our resuslts will pit two Disney songs against one another and you will choose which was written by Ashman and which was not so that Alan can decide if he'd like his old partner back as an AI.  \n",
        "\n",
        "</br>\n",
        "\n",
        "## **Here Are a couple of real songs from Ashman and Menken** \n",
        "\n",
        "### **Part of Your World**\n",
        "<p>\n",
        "Look at this stuff, isnt it neat? </br> \n",
        "Wouldn't you think my collection's complete? </br>\n",
        "Wouldn't you think Im the girl,\n",
        "the girl who has everything?\n",
        "</p>\n",
        "\n",
        "\n",
        "![\"Part of Your World\"](https://media.giphy.com/media/ok1tOvpe5yvy8/giphy.gif)\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "### **A Whole New World**\n",
        "<p>\n",
        "I can show you the world: shining, shimmering, splendid!  </br> \n",
        "Tell me Princess, now when did you last let your heart decide? \n",
        "</p>\n",
        "\n",
        "![\"A Whole New World\"](https://media.giphy.com/media/jyJKxPKMTy36w/giphy.gif)\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **(Not Ashman and Menken, but due to the dataset we get the old family classic) Lester's Possum Park**\n",
        "<p>\n",
        "Now gather 'round my possum pals, join the jamboree! </br>\n",
        "Come hoot and howl and holler from the heart</br>\n",
        "Every chicken, pig ,and goat will help </br>\n",
        "by yelpin' out a yodel here at <br>\n",
        "Lester's Possum Park!\n",
        "\n",
        "</p>\n",
        "\n",
        "![\"Lester's Possum Park\"](https://media.giphy.com/media/n1NKE2VJ0h6MM/giphy.gif)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwitJAQ5hkv-",
        "colab_type": "text"
      },
      "source": [
        "## **What we got:**  \n",
        "Givin the current state of AI unupervised text generation models, and the  dataset for used for pretraining (us),  What Alan Menken is actually in store for is probably interesting but not quite what he's looking for.\n",
        "\n",
        "**My Burrito's Whole Funky Sloth Park (anyone?)**\n",
        "<p>\n",
        "Just Imagine what these lyrics might say... I assure you it was worse. \n",
        "\n",
        "</p>\n",
        "\n",
        "![\"Burrito's New Sloth Park\"](https://media.giphy.com/media/x4gaYH1e2OsVO/giphy.gif) \n",
        "\n",
        "</br>\n",
        "\n",
        "But then it got better, and we will explore results at the end! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsq_lWQGmQrp",
        "colab_type": "text"
      },
      "source": [
        "# **Data**\n",
        "\n",
        "The first thing we need are some Disney song lyrics! Unfortunately for both Alan and I, Disney owns the rights to those lyrics, so we had to rely on scraped lyrics from [All the Lyrics ](https://allthelyrics.com), which can be tricky as people input these lyrics. \n",
        "The first step was to write a scraper that used Beautiful Soup to grab the lyrics of every song on the site tagged \"Disney\" and write the results to a .csv file, \"songs.csv\".  \n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVtyRx525NdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# Imports \n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "import urllib.request\n",
        "import collections\n",
        "import csv\n",
        "import logging\n",
        "import re\n",
        "import sys\n",
        "\n",
        "BASE_URL = 'https://www.allthelyrics.com'\n",
        "FILTER_ELS = '/lyrics/disney'\n",
        "LANGS = ('en', )\n",
        "CSV_OUT = 'songs.csv'\n",
        "SEP = ','\n",
        "\n",
        "# Python namedtuple\n",
        "Song = collections.namedtuple('Song', 'album title lyrics')\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "\n",
        "def validate_en(lyrics):\n",
        "    \"\"\"Validate that song lyrics are written in English\n",
        "\n",
        "    Arguments:\n",
        "        lyrics (str): Song lyrics\n",
        "\n",
        "    Returns:\n",
        "        bool: Whether the lyrics are English or not\n",
        "    \"\"\"\n",
        "\n",
        "    l = detect(lyrics)\n",
        "    logging.info('Language: {}'.format(l))\n",
        "    if l in LANGS:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def get_lyrics_data(url):\n",
        "    \"\"\"Fetch and extract lyrics data from an all the lyrics URL\n",
        "\n",
        "    Arguments:\n",
        "        url (str): an AllTheLyrics song URL\n",
        "\n",
        "    Returns: \n",
        "        Song: A named tuple containing song lyric data\n",
        "    \"\"\"\n",
        "    source = urllib.request.urlopen(url).read()\n",
        "    soup = BeautifulSoup(source, 'html.parser')\n",
        "    title, album, lyrics = None, None, None\n",
        "\n",
        "    # song title\n",
        "    # get page title first...\n",
        "    title = soup.title.string\n",
        "    # then sanitize page title into song title\n",
        "    title = re.sub('^Disney â€“ ', '', title)\n",
        "    title = re.sub(' lyrics', '', title)\n",
        "    logging.info('Title: {}'.format(title))\n",
        "\n",
        "    # album\n",
        "    for a in soup.find_all('div', class_='content-text-album'):\n",
        "        album = a.text\n",
        "        # strip leading string from album div text\n",
        "        album = re.sub('Album: ', '', album)\n",
        "    logging.info('Album {}'.format(album))\n",
        "\n",
        "    # lyrics\n",
        "    for d in soup.find_all('div', class_='content-text-inner'):\n",
        "        lyrics_raw = d.text\n",
        "        if validate_en(lyrics_raw):\n",
        "            lyrics = lyrics_raw\n",
        "            # Some songs lyrics are written(character)\n",
        "            #Lyric which will confuse the system. Add a space to fix this.\n",
        "            lyrics = re.sub('\\)', ') ', lyrics)\n",
        "            lyrics = re.sub('[ ]{1,}', ' ', lyrics)\n",
        "            # Some songs have character :\n",
        "            # lyric -OR- (character) : lyric. \n",
        "            # Drop leding space from colon character\n",
        "            lyrics = re.sub(' :', ':', lyrics)\n",
        "            # Some songs have [character:] lyric. \n",
        "            # Drop the internal colon character.\n",
        "            lyrics = re.sub(':] ', '] ', lyrics)\n",
        "            return Song(album, title, lyrics)\n",
        "        else:\n",
        "            raise ValueError('Non-English song')\n",
        "\n",
        "\n",
        "def fetch_urls_from_index(url):\n",
        "    \"\"\"Fetch song links from an AllTheLyrics index URL\n",
        "    \n",
        "    Arguments:\n",
        "        url (str): an AllTheLyrics index URL\n",
        "\n",
        "    Returns: \n",
        "        list: A list of AllTheLyrics song URLs\n",
        "    \"\"\"\n",
        "    urls = []\n",
        "    source = urllib.request.urlopen(url).read()\n",
        "    soup = BeautifulSoup(source, 'html.parser')\n",
        "    for t in soup.find_all('a'):\n",
        "        href = t.get('href')\n",
        "        # all lyrics pages start with /lyrics\n",
        "        if href.startswith('/lyrics'):\n",
        "            urls.append(BASE_URL + href)\n",
        "\n",
        "    logging.info('Candidate URLs: {}'.format(len(urls)))\n",
        "    return urls\n",
        "\n",
        "\n",
        "def main():\n",
        "    song_count = 0\n",
        "    with open(CSV_OUT, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile, delimiter=SEP, quoting=csv.QUOTE_MINIMAL)\n",
        "        for u in fetch_urls_from_index(BASE_URL + FILTER_ELS):\n",
        "            try:\n",
        "                song = get_lyrics_data(u)\n",
        "                writer.writerow(list(song))\n",
        "                song_count = song_count + 1\n",
        "            except Exception:\n",
        "                logging.exception('Skipped')\n",
        "\n",
        "    logging.info('Songs extracted: {}'.format(song_count))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sys.exit(main())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iakn7r5h6pmM",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning\n",
        "I had to do a bit of hand cleaning and curation on the data, which were 417 complete sets of lyrics in order to: \n",
        "* Ensure the songs actually were Disney songs. (nearly all were, but some VERY obscure) \n",
        "* Fill in as much of the missing information as possible:  Some songs were missing titles, some had incorrect titles and many were missing the source of  the song.  \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPsdA_EL_Lp2",
        "colab_type": "text"
      },
      "source": [
        "# **So what do we need to do to train this data?**\n",
        "\n",
        "![Straight Shooter](https://media.giphy.com/media/F0DHF9GOM0DzG/giphy.gif)\n",
        "\n",
        "We need to give the  data to the model straight, so we don't confuse it...too much. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW_XAwfi7njB",
        "colab_type": "text"
      },
      "source": [
        "### Transforming and Formatting \n",
        "The Model, as we wil see later, needs the songs needed to be added as a .txt file so it will have an idea of what we want back. We need to give it an explicit template:  </br>\n",
        "\n",
        "TITLE: Insert title here </br>\n",
        "\n",
        "Lyrics</br>\n",
        "Lyrics</br>\n",
        "Lyrics</br>\n",
        "<p>--- </br>\n",
        "The following formatter accomplished this task nicely!  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42pqlFYn-Xmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Formatter\n",
        "\n",
        "Transforms the songs.csv data set into a single text file\n",
        "suitable for fine-tuning a GPT2 model. The format is as follows:\n",
        "\n",
        "SOURCE - Source here \n",
        "TITLE: Insert title here\n",
        "\n",
        "Lyrics\n",
        "Lyrics\n",
        "Lyrics\n",
        "---\n",
        "\n",
        "\"\"\"\n",
        "#!/usr/bin/env python\n",
        "\n",
        "import csv\n",
        "import sys\n",
        "\n",
        "CSV_IN = 'songs.csv'\n",
        "SEP = ','\n",
        "TEXT_OUT = 'songs.txt'\n",
        "\n",
        "def main():\n",
        "  with open(TEXT_OUT, 'w') as text_file:\n",
        "      with open(CSV_IN, 'r', newline='') as csvfile:\n",
        "          reader = csv.reader(csvfile, delimiter=SEP, quoting=csv.QUOTE_MINIMAL)\n",
        "          for row in reader:\n",
        "              text_file.write('SOURCE - {}\\n\\n'.format(row[0]))\n",
        "              text_file.write('TITLE: {}\\n\\n'.format(row[1]))\n",
        "              text_file.write('{}---\\n\\n'.format(row[2]))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sys.exit(main())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2kodhYRByGF",
        "colab_type": "text"
      },
      "source": [
        "## Now we have a nice shiny text file, ready to be used for training our model! Here's an example:  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmTT-oalCY4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "TITLE: A Whole New World\n",
        "\n",
        "(Aladdin) I can show you the world\n",
        "Shining, shimmering, splendid\n",
        "Tell me, princess, now when did\n",
        "You last let your heart decide?\n",
        "I can open your eyes\n",
        "Take you wonder by wonder\n",
        "Over, sideways and under\n",
        "On a magic carpet ride\n",
        "A whole new world\n",
        "A new fantastic point of view\n",
        "No one to tell us no\n",
        "Or where to go\n",
        "Or say we're only dreaming\n",
        "(Jasmine) A whole new world\n",
        "A dazzling place I never knew\n",
        "But when I'm way up here\n",
        "It's crystal clear\n",
        "that now I'm in a whole new world with you\n",
        "(Aladdin) Now I'm in a whole new world with you\n",
        "(Jasmine) Unbelievable sights\n",
        "Indescribable feeling\n",
        "Soaring, tumbling, freewheeling\n",
        "Through an endless diamond sky\n",
        "(Jasmine) A whole new world\n",
        "(Aladdin) Don't you dare close your eyes\n",
        "(Jasmine) A hundred thousand things to see\n",
        "(Aladdin) Hold your breath - it gets better\n",
        "(jasmine) I'm like a shooting star\n",
        "I've come so far\n",
        "I can't go back to where I used to be\n",
        "(Aladdin) A whole new world\n",
        "(Jasmine) Every turn a surprise\n",
        "(Aladdin) With new horizons to pursue\n",
        "(Jasmine) Every moment gets better\n",
        "(Both) I'll chase them anywhere\n",
        "There's time to spare\n",
        "Let me share this whole new world with you\n",
        "(Aladdin) A whole new world\n",
        "(Jasmine) A whole new world\n",
        "(Aladdin) That's where we'll be\n",
        "(Jasmine) That's where we'll be\n",
        "(Aladdin) A thrilling chase\n",
        "(Jasmine) A wondrous place\n",
        "(Both) For you and me\n",
        "---\n",
        "\n",
        "..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8-HPVzfEzdr",
        "colab_type": "text"
      },
      "source": [
        "![What Do You Need?](https://media.giphy.com/media/t7NqkPey14axG/giphy.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKg1BCXRcWuz",
        "colab_type": "text"
      },
      "source": [
        "#**Training The Model**\n",
        "\n",
        "## Model : I used a Language Model called a Transformer.\n",
        "![Transitron from Toy Story of Terror!](https://vignette.wikia.nocookie.net/pixar/images/0/03/TransitronToyStoryOfTerror1.png/revision/latest?cb=20131019015524)</br>\n",
        "(I desperately wanted Transformers to be owned by Disney.  They are not. However, Transitron does exist in the [Pixar Toy Story Universe](https://pixar.fandom.com/wiki/Transitron), and is therefore fair game! )\n",
        "</br>\n",
        "(**Not That one though!  THIS one:**)</br>\n",
        "![Transformer](https://miro.medium.com/max/2254/1*Aqcm4iX3AQNWx9Zb-z7o1Q.png)\n",
        "\n",
        "## A Language Model is basically a machine learning model that is able to look at part of a sentence and predict the next word.</br>\n",
        "The most famous language models are smartphone keyboards that suggest the next word based on what youâ€™ve currently typed.\n",
        "\n",
        "**The Transformer** is a [model](https://towardsdatascience.com/transformers-141e32e69591) that uses attention to boost the speed with which Language models can be trained.\n",
        "\n",
        "GPT-2 (a successor to GPT), is one of several Lanuage models. \n",
        "* large transformer-based language model with 1.5 billion parameters\n",
        "* trained on a dataset of over 8 million web pages \n",
        "* trained simply to predict the next word in 40GB of Internet text.</br> \n",
        "* simple objective: predict the next word, given all of the previous words within some text. </br>\n",
        "\n",
        "## [From the GPT-2 Team- ](https://openai.com/blog/better-language-models/)\n",
        "> We created a new dataset which emphasizes diversity of content, by scraping content from the Internet. In order to preserve document quality, we used only pages which have been curated/filtered by humansâ€”specifically, we used outbound links from Reddit which received at least 3 karma. This can be thought of as a heuristic indicator for whether other users found the link interesting (whether educational or funny), leading to higher data quality than other similar datasets, such as CommonCrawl.</br>\n",
        "\n",
        "(**They used Reddit! What could possibly go wrong?**)\n",
        "\n",
        "> GPT-2 generates synthetic text samples in response to the model being primed with an arbitrary input. The model is chameleon-likeâ€”it adapts to the style and content of the conditioning text. This allows the user to generate realistic and coherent continuations about a topic of their choosing\n",
        "\n",
        "\n",
        "## [The Illustrated GPT-2 (Visualizing Transformer Language Models)](http://jalammar.github.io/illustrated-gpt2/)\n",
        "\n",
        "\n",
        "## **How do I get my hands on this?**\n",
        "\n",
        "I used a Library called [gpt-2-simple](https://github.com/minimaxir/gpt-2-simple) by Max Woolf and spun out my own Colab notebook to do my training. \n",
        "\n",
        "* spin up a single google colaboratory notebook\n",
        "* load various sizes of the GPT-2 model (I used 355 MB) \n",
        "* fine-tune using TensorFlow \n",
        "* complimentary GPU provided by google colaboratory\n",
        "* generate synthetic text passages \n",
        " \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "## How to Train a GPT-2 Text-Generating Model w/ GPU \n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset on a GPU using Collaboratory using `gpt-2-simple` library.\n",
        "\n",
        "For more about `gpt-2-simple`, visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple). See here: [blog post](https://minimaxir.com/2019/09/howto-gpt2/) for more information. \n",
        "\n",
        "* Import tensorflow V1.x (gpt-2-simple only uses 1 )\n",
        "* Install the library gpt-2-simple\n",
        "* import datetime\n",
        "* import needed files \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## About the GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing me to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "Verify which GPU is active after running the cell.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "Since I am retraining a model on new text (the lyrics), I need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:</br>\n",
        "#### Small\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.</br>\n",
        "\n",
        "#### Medium: I chose this size\n",
        "\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.</br>\n",
        "\n",
        "#### Large and XL:  These two cannot be effectively fine-tuned on one GPU in colab </br>\n",
        "(These Larger models have more knowledge, but take longer to finetune and longer to generate text.) \n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        " I specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "Below, gpt-2 is downloaded from Google Cloud Storage and saved in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; I have to redownload it if I want to retrain it at a later time.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"355M\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount my personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, I select *Files*. From there I upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"songs.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If the text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(\"songs.txt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "My next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (If I want to have the finetuning run indefinitely, I can set `steps = -1`. I will not do that here.)\n",
        "\n",
        "The model checkpoints are saved in `/checkpoint/run1`. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "(The training might time out after 4ish hours; I need to make sure I end training and save the results so I don't lose them!)\n",
        "\n",
        "(**IMPORTANT NOTE:** To rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). I need to Rerun imports but I do not need to recopy files.\n",
        "\n",
        "## optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Result Interp: \n",
        "#  (ie) [10 | 28.15] loss=3.44 avg=3.44\n",
        "#  [NUMBER| TIME] loss=LOSS avg=AVG \n",
        "#1: NUMBER - Refers to the number of training step. Think of it as a counter that will increase by 1 after each run.\n",
        "#2: TIME - elapsed since the start of training in seconds. \n",
        "#   We can use the first step as reference to determine how long does it take to run one step.\n",
        "#3: LOSS and AVG -  Both of them refer to the cross-entropy (log loss) and the average loss.\n",
        "#   We can use this to determine the performance of the model.\n",
        "#   In theory, as training steps increases,the loss should decrease until it converges at certain value. \n",
        "#   The lower, the better.\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='355M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              # latest\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, copy the checkpoint folder to my Google Drive.\n",
        "\n",
        "(To download it to my personal computer, copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file;It may be downloaded and uncompressed locally.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "Done with setup! Next is the **Generate Text From The Trained Model** section to generate text based on the retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Next, I copy the `.rar` checkpoint file from my Google Drive into the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_from_gdrive(run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** To rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). I will need to rerun imports but not recopy files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After training the model or loading a retrained model from checkpoint, I can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess, run_name='run1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "To create an API based on a model and to pass the generated text elsewhere, one can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "One can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if  adding an indicator when the text starts).\n",
        "\n",
        "One can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, one can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "### Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
        "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess, \n",
        "              run_name='run4',\n",
        "              prefix='SOURCE - ', \n",
        "              temperature=0.8,\n",
        "              truncate='---',\n",
        "              nsamples=10,\n",
        "              batch_size=10,\n",
        "              top_k=40)\n",
        "\n",
        "# original  \n",
        "# gpt2.generate(sess,\n",
        "#               length=250,\n",
        "#               temperature=0.7,\n",
        "#               prefix=\"TITLE: \",\n",
        "#               nsamples=5,\n",
        "#               batch_size=5\n",
        "#               )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "source": [
        "For bulk generation, generate a large amount of text to a file and sort out the samples on a local computer. The next cell will generate a generated text file with a unique timestamp.\n",
        "\n",
        "Rerun the cells as many times as you want for even more generated texts!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LRex8lfv1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# may have to run twice to get file to download\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQAN3M6RT7Kj",
        "colab_type": "text"
      },
      "source": [
        "## Generate Text From The Pretrained Model\n",
        "\n",
        "To generate text from the pretrained model, not a finetuned model, pass `model_name` to `gpt2.load_gpt2()` and `gpt2.generate()`.\n",
        "\n",
        "This is currently the only way to generate text from the 774M or 1558M models with this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsUd_jHgUZnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = \"774M\"\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAe4NpKNUj2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.load_gpt2(sess, model_name=model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xInIZKaU104",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              model_name=model_name,\n",
        "              prefix=\"The secret of life is\",\n",
        "              length=100,\n",
        "              temperature=0.7,\n",
        "              top_p=0.9,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V67k3-8AEMlX",
        "colab_type": "text"
      },
      "source": [
        "# **Results**\n",
        "----------\n",
        "\n",
        "There were pages of results, but I hand picked a few to illustrate the stages. \n",
        "## **Early runs:**\n",
        "\n",
        "Repetition\n",
        "```\n",
        "TITLE: Tiki\n",
        "Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki\n",
        "Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki\n",
        "Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki Tiki....\n",
        "```\n",
        "\n",
        "Nearly verbatim lyrics from The Aristocats, but adds a flourish here and there.\n",
        "```\n",
        "SOURCE -  The Aristocats\n",
        "TITLE: The Aristocats\n",
        "Which pet's address\n",
        "Is the finest in Paris?\n",
        "Which pet possess\n",
        "The longest pedigree?\n",
        "Which pets get to sleep\n",
        "On velvet mats?\n",
        "Which pets get a royal bath?\n",
        "Which pets get to comb their\n",
        "own voluptuouses?  <<< Totally made up word\n",
        "```\n",
        "gpt-2 drawing from a vast array of scientific knowledge to complete the Disnified lyrics, and odd capitalization\n",
        "```\n",
        "SOURCE -  Wreck-It Ralph\n",
        "REMEMBER WHEN I SAID THAT WE'D GO SCIENCE FICTION? << Odd caps\n",
        "Ralph is on the march again\n",
        "Chasing a Scoville index\n",
        "From Zero to 200\n",
        "Fast-forward to today\n",
        "And He's sold more than a billion books!\n",
        "```\n",
        "Combining Disney songs, but adding it's own flourish of gaming speak.\n",
        "```\n",
        "TITLE: I Just Can't Wait To Be King\n",
        "Simba: I'm gonna be a mighty king\n",
        "Just a mighty king\n",
        "Just a mighty king\n",
        "Saying: Be my guest\n",
        "A Pint of the New Moon!\n",
        "We surrender to your majesty!\n",
        "Yes, we do.\n",
        "\n",
        "```\n",
        "Mary (Ice- )Poppins? Let it Go? \n",
        "```\n",
        "TITLE: Supercalifragilisticexpialidocious  <<< Got it right! \n",
        "When an ice storm hits London,\n",
        "And the ice doesn't last,\n",
        "The ice queen will come to town.\n",
        "She'll whip up a super-sized sleet,\n",
        "That's super-super-super-super-cool. \n",
        "```\n",
        "\n",
        "Outtakes: </br>\n",
        "It just can't seem to make up it's mind! More repetition\n",
        "```\n",
        "TITLE: What's My Name?\n",
        "What's my name?\n",
        "What's my name?\n",
        "What's my name?\n",
        "What's my name?\n",
        "```\n",
        "Misspelling, but bonus points for use of \"thy\"? \n",
        "```\n",
        "TITLE: Good Night\n",
        "Good night\n",
        "Don't delay\n",
        "thy morning report  << Props for Shakesperian Lion King though\n",
        "thy good brook\n",
        "thy shore's free\n",
        "all around lovely *blossombed* << b \n",
        "Tranquil as a child\n",
        "Tranquil as a child\n",
        "Good night\n",
        "```\n",
        "\n",
        "\n",
        "## **Mid runs:**\n",
        "No. One. Else. Wrote. This. \n",
        "```\n",
        "TITLE: Everything You Wanted (Only Elton John)  << ONLY EJ\n",
        "'Cause everything we see, we must believe  << Starts with 'Cause. Who else? \n",
        "And the things we see, we must believe\n",
        "This is the life we've been dreaming of\n",
        "A life we can share\n",
        "A simple life, as beautiful as it is wild\n",
        "This is the life we want\n",
        "A life we can share\n",
        "A simple life, as beautiful as it is wild\n",
        "Watch how we move, feel, and sing\n",
        "Listen to the words that I'm singing\n",
        "And feel the smile that comes to you\n",
        "Just let me in, I'll show you how to be\n",
        "Wild as all you want to be\n",
        "\n",
        "```\n",
        "Dear Abby...er gpt-2: Can I sum it up right here. (This is straight from reddit)\n",
        "```\n",
        "TITLE: Heartfelt   <<AWWWW \n",
        "tl;dr: I lost my way, and everyone looks like me  \n",
        "<<<<<<<<<<<WE CAN TOTALLY SKIP THE REST>>>>>>>>>>>>>>\n",
        "So I give myself a wide berth,\n",
        "because I'm too trusting,\n",
        "and I let the people I love tell me who they want to be.\n",
        "They don't have to know who I am,\n",
        "or where my heart is,\n",
        "they can just see me do what I do.\n",
        "I roll with the punches,\n",
        "I sit back and wait for my turn,\n",
        "when I get punched in the gut,\n",
        "I get up and move on.\n",
        "I see the world through,\n",
        "and I see it differently,\n",
        "I see the world through,\n",
        "and I see there's nothing I can't do.\n",
        "I roll with the punches,\n",
        "I sit back and wait for my turn,\n",
        "when I get punched in the gut,\n",
        "I get up and move on.\n",
        "I see the world through,\n",
        "and I see it differently,\n",
        "I see the world through.\n",
        "And I see you,\n",
        "here on my record,\n",
        "I see you,\n",
        "here on my conscience,\n",
        "I see myself through,\n",
        "and I see you\n",
        "...everyday.\n",
        "\n",
        "```\n",
        "Is \"Derek\" being PA about \"Who\" vs \"Whom\" in this version of Jungle Book? ( Also from Reddit?)\n",
        "```\n",
        "TITLE: I Don't wanna be like you\n",
        "Derek:  \n",
        "Look you can't be friends with him\n",
        "Me?\n",
        "Both:\n",
        "Friends with whom?\n",
        "Derek:\n",
        "With whom?\n",
        "Me:\n",
        "With whom?\n",
        "Derek:\n",
        "With whom?\n",
        "Me:\n",
        "Who?   << Yeah, I said it!  \n",
        "Derek:\n",
        "With whom? \n",
        "Me:\n",
        "Whoever it may be. << Screw you, Derek. \n",
        "Derek and Me:\n",
        "We want very much to be different.\n",
        "We want to be perfect for each other.\n",
        "Like Derek, Me and Derek.\n",
        "Our relationship will not survive.\n",
        "Me:\n",
        "Will you just let me go...\n",
        "Derek:\n",
        "Look at him. I don't know what to do.\n",
        "Me:\n",
        "Leave him be. He'll learn.\n",
        "Derek:\n",
        "What to do?\n",
        "Me:\n",
        "What to do?\n",
        "Derek:\n",
        "Screw you.   <<< DID HE JUST???? \n",
        "Me:\n",
        "I can't.\n",
        "I can't do it.\n",
        "I can't do it straight away.\n",
        "I can't do it at all.\n",
        "I can't do it in one go.\n",
        "So please, somebody, tell me what to do.  << LEAVE HIM\n",
        "Something's gotta give tonight.\n",
        "It's my last one before I'm buried.\n",
        "So if you're counting sheep, I'm counting goats.\n",
        "And I'm counting on Derek to be my shepherd.\n",
        "My sheep are restless, sad heads need a rest.\n",
        "My goats are skinny yips, so I'm counting on Derek to be my shepherd.  <<<  OH NO!!  \n",
        "My sheep are restless, sad heads need a rest.\n",
        "And I'm counting on Derek to be my shepherd.\n",
        "I'm counting on Derek to be my shepherd.  \n",
        "\n",
        "(DONT DO IT, \"Me\"!) \n",
        "```\n",
        "\n",
        "\n",
        "## **Late runs:**\n",
        "Religiousity?    \n",
        "```\n",
        "TITLE: Theres A Party In Agrabah Part 2  << Part 2 \n",
        "HEBREWS   \n",
        "(Gasps with excitement) << !! \n",
        "JASMINE\n",
        "You found him!\n",
        "GARDNER\n",
        "What now?\n",
        "JASMINE\n",
        "I'm giving him an ultimatum!\n",
        "GARDNER\n",
        "If he doesn't give me back the honey, I'll have to find a new way to eat!\n",
        "JASMINE\n",
        "(Neutral) \n",
        "```\n",
        "Bare Nessesities: LifeTip DudeBro 2020 ( This isn't at all what I remembered Baloo imparting to Mowgli)\n",
        "```\n",
        "TITLE: The Bare Necessities\n",
        "I've been around the block before\n",
        "Stood in the back and avoided being hit\n",
        "'Cause I've got something I learned from 'em.\n",
        "The bare nessesities.\n",
        "Like getting up in the morning and straightening up\n",
        "'Cause those simple bare nessesities\n",
        "Reduce stress and increase energy\n",
        "And when you're stressed, it's great relief\n",
        "To know that your energy levels will return to normal once you start bare nessing.\n",
        "Once a week I'll double up as a lab rat\n",
        "And I'll stick it out as long as I can\n",
        "Because I've got something I learned from 'em.\n",
        "The bare nessesities.\n",
        "Like getting up in the morning and straightening up\n",
        "'Cause those simple bare nessesities\n",
        "Reduce stress and increase energy\n",
        "And when you're stressed, it's great relief\n",
        "To know that your energy levels will return to normal once you start bare nesses.\n",
        "Once a week I'll double up as a lab rat\n",
        "And I'll stick it out as long as I can\n",
        "Because I've got something I learned from 'em.\n",
        "\n",
        "```\n",
        "Robin Williams gets an attribution for this amaglemation that sports some Hunchback refrerences and a very Tinkerbelle Movie \"Foolharyette\"! At Least there are no \"Flubber\" Refs!  \n",
        "```\n",
        "TITLE: Let Me Be Your Wings\n",
        "Here I stand, alone and wide-eyed\n",
        "In a burning building full of fire\n",
        "Never dreaming why\n",
        "All my dreams are like stars in the sky\n",
        "Now here I stand, holding the sword in my fist\n",
        "And the voice inside sings a song\n",
        "Of fear and anger and love\n",
        "And trust and kinship\n",
        "And all it takes is one syllable\n",
        "To say...\n",
        "Let me be your wings  <LETMEBEYOURWINGS\n",
        "Have you ever heard the wolf cry?\n",
        "Or asked the smiling cow for milk\n",
        "Or asked a dumbbells-n'-tenpenny how many pounds\n",
        "You can carry?\n",
        "Because if you haven't, you're a fool\n",
        "and a foolhardyette\n",
        "you should learn to be a fool\n",
        "-- Robin Williams << \n",
        "\n",
        "```\n",
        "Presented without comment, but I know what you are all thinking. \n",
        "```\n",
        "TITLE: The American Adventure\n",
        "Tell us what to do...\n",
        "\n",
        "(<End of Song>)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## **Funny Titles**\n",
        "```\n",
        "TITLE: You Can Fly It's Just A Question Of When\n",
        "TITLE: I'm Still Here (O Brother)\n",
        "TITLE: I Spy (Rapunzel's Thing)\n",
        "TITLE: I'm On Your Mind \n",
        "```\n",
        "\n",
        "\n",
        "## **Bonus Round:  Titles from Prompt**\n",
        "\n",
        "```\n",
        "(Prefix = 'Title: Toss a Coin to Your Witcher')\n",
        "\n",
        "Witcher Rap\n",
        "TITLE: Toss a Coin to Your Witcher Hat\n",
        "(bonus lyrics)\n",
        "Do that for me!! Want a thrill???\n",
        "Throw ya middle button. \n",
        "Hit mumble!\n",
        "Don't let the middle card go unrewarded.\n",
        "\n",
        "\n",
        " (Now, That's some life advice if ever I heard it! Thanks gpt-2) \n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Which is Disney?  NO CHEATING! \n",
        "```\n",
        "TITLE: My Love Will Find A Way\n",
        "My love will find a way\n",
        "To make me whole\n",
        "Again\n",
        "Again\n",
        "Though he walked away\n",
        "Our song was a prayer\n",
        "We put the fear of God into you\n",
        "And made you understand\n",
        "What love is\n",
        "My heart has wings and can fly\n",
        "My love will find a way\n",
        "To make me whole\n",
        "Again\n",
        "Again\n",
        "```\n",
        "\n",
        "```\n",
        "TITLE: The Good Life\n",
        "When I get to know him\n",
        "For a time there I'll be safe and happy\n",
        "Just let him be\n",
        "Just let him be\n",
        "Because I know he'll be there\n",
        "When I get to know him better\n",
        "For a time there I'll be safe and happy\n",
        "Just let him be\n",
        "Just let him be\n",
        "Because I know he'll be there\n",
        "```\n",
        "## What do we think?  Should we recommend that Alan use this new trained model to help him? \n",
        "\n",
        "![Tiki Tiki](https://media.giphy.com/media/3ohze3TuWAtISYN6c8/giphy.gif) \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzlEnd-39_dE",
        "colab_type": "text"
      },
      "source": [
        "![Applause!](https://media.giphy.com/media/BYaQgGmxF4bgA/giphy.gif)\n",
        "\n",
        "# **Thank You:**  \n",
        "##Matt Vaughn </br>\n",
        "##Max Woolf</br>\n",
        "##Manuel Lara</br>\n",
        "##Dylan Rossi  </br>\n",
        "##Wenbin Xiao </br>\n",
        "##Anselmo Jr Garza</br>\n",
        "## Amy Koldeway, Sarah Cross and my classmates \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Fun Fact:** **I trained with the same Voice teacher at [the same music school](https://millikin.edu/original-ariel-voice-launched-millikin) as Jodi Benson (aka Little Mermaid) which is why I'm so interested in Disney, Howard Ashman and Alan Menken**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}